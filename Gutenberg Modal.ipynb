{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gutenbern corpus with NLTK package\n",
    "### Step 1: Download and Install Gutenberg Corpus\n",
    "\n",
    "    Install NLTK: run sudo pip install -U nltk\n",
    "    Install Numpy (optional): run sudo pip install -U numpy\n",
    "    Test installation: run python then type import nltk\n",
    "\n",
    "### Step 2: Import source data from the package NLTK\n",
    "#### The Brown Corpus was the first million-word electronic corpus of English, created in 1961 at Brown University. This corpus contains text from 500 sources, and the sources have been categorized by genre, such as news, editorial, and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk as NLTK\n",
    "import itertools as ListOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "cats = brown.categories()\n",
    "print(cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Relative Frequency distributionand conditional frequency of modals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency Distribution of modals \n",
      "\n",
      "can: 94 could: 87 may: 93 might: 38 will: 389 would: 246 should: 61 "
     ]
    }
   ],
   "source": [
    "#Frequency Distribution\n",
    "text = brown.words(categories='news')\n",
    "fdist = NLTK.FreqDist(w.lower() for w in text)\n",
    "modals = ['can', 'could', 'may', 'might', 'will', 'would', 'should']\n",
    "print('Frequency Distribution of modals \\n')\n",
    "for mods in modals:\n",
    "    print(mods + ':', fdist[mods], end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional frequency Distribution for modals across all genres\n",
      "\n",
      "                   can  could    may  might   will  would should \n",
      "      adventure     46    151      5     58     50    191     15 \n",
      " belles_lettres    246    213    207    113    236    392    102 \n",
      "      editorial    121     56     74     39    233    180     88 \n",
      "        fiction     37    166      8     44     52    287     35 \n",
      "     government    117     38    153     13    244    120    112 \n",
      "        hobbies    268     58    131     22    264     78     73 \n",
      "          humor     16     30      8      8     13     56      7 \n",
      "        learned    365    159    324    128    340    319    171 \n",
      "           lore    170    141    165     49    175    186     76 \n",
      "        mystery     42    141     13     57     20    186     29 \n",
      "           news     93     86     66     38    389    244     59 \n",
      "       religion     82     59     78     12     71     68     45 \n",
      "        reviews     45     40     45     26     58     47     18 \n",
      "        romance     74    193     11     51     43    244     32 \n",
      "science_fiction     16     49      4     12     16     79      3 \n"
     ]
    }
   ],
   "source": [
    "cFrqDist = NLTK.ConditionalFreqDist((genre, word)\n",
    "                               for genre in brown.categories()\n",
    "                               for word in brown.words(categories=genre))\n",
    "genres = cats\n",
    "print('Conditional frequency Distribution for modals across all genres\\n')\n",
    "cFrqDist.tabulate(conditions=genres, samples=modals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inaugural corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import inaugural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text files are: \n",
      " ['1789-Washington.txt', '1793-Washington.txt', '1797-Adams.txt', '1801-Jefferson.txt', '1805-Jefferson.txt', '1809-Madison.txt', '1813-Madison.txt', '1817-Monroe.txt', '1821-Monroe.txt', '1825-Adams.txt', '1829-Jackson.txt', '1833-Jackson.txt', '1837-VanBuren.txt', '1841-Harrison.txt', '1845-Polk.txt', '1849-Taylor.txt', '1853-Pierce.txt', '1857-Buchanan.txt', '1861-Lincoln.txt', '1865-Lincoln.txt', '1869-Grant.txt', '1873-Grant.txt', '1877-Hayes.txt', '1881-Garfield.txt', '1885-Cleveland.txt', '1889-Harrison.txt', '1893-Cleveland.txt', '1897-McKinley.txt', '1901-McKinley.txt', '1905-Roosevelt.txt', '1909-Taft.txt', '1913-Wilson.txt', '1917-Wilson.txt', '1921-Harding.txt', '1925-Coolidge.txt', '1929-Hoover.txt', '1933-Roosevelt.txt', '1937-Roosevelt.txt', '1941-Roosevelt.txt', '1945-Roosevelt.txt', '1949-Truman.txt', '1953-Eisenhower.txt', '1957-Eisenhower.txt', '1961-Kennedy.txt', '1965-Johnson.txt', '1969-Nixon.txt', '1973-Nixon.txt', '1977-Carter.txt', '1981-Reagan.txt', '1985-Reagan.txt', '1989-Bush.txt', '1993-Clinton.txt', '1997-Clinton.txt', '2001-Bush.txt', '2005-Bush.txt', '2009-Obama.txt', '2013-Obama.txt', '2017-Trump.txt'] \n",
      " \n",
      " The number of text files are:  58\n"
     ]
    }
   ],
   "source": [
    "inaguralCats = inaugural.fileids()\n",
    "print('The text files are: \\n', inaguralCats, '\\n \\n The number of text files are: ', len(inaguralCats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/raam/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "NLTK.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining words from all 58 text files of inaugural \n",
      "\n",
      "Number of words before treating the list with Stopwords:  149797 \n",
      " --------\n",
      "Number of words after implementing conditions and cleaning the list with Stopwords:  22762\n"
     ]
    }
   ],
   "source": [
    "print('Combining words from all', len(inaguralCats), 'text files of inaugural \\n')\n",
    "\n",
    "def getWords(x):\n",
    "    return(inaugural.words(fileids=x))\n",
    "\n",
    "allWordsFromInagural = list(map(getWords, inaguralCats))\n",
    "\n",
    "#To join nested lists of allWordsFromInagural\n",
    "allWordsFromInagural = list(ListOps.chain.from_iterable(allWordsFromInagural))\n",
    "\n",
    "print('Number of words before treating the list with Stopwords: ', len(allWordsFromInagural), '\\n --------')\n",
    "\n",
    "# To filter words based on the mentioned conditions\n",
    "allWordsFromInagural = list(filter(lambda x: x not in stopwords.words('english') and x.isalpha() == True and \n",
    "                                   len(x) > 7, allWordsFromInagural))\n",
    "\n",
    "print('Number of words after implementing conditions and cleaning the list with Stopwords: ', \n",
    "      len(allWordsFromInagural))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 frequently used words across all the text files are: \n",
      " ['government', 'citizens', 'constitution', 'national', 'congress', 'interests', 'political', 'executive', 'principles', 'progress']\n"
     ]
    }
   ],
   "source": [
    "inaguralFDist = NLTK.FreqDist(everyWord.lower() for everyWord in allWordsFromInagural)\n",
    "\n",
    "topTen = dict(map(lambda x: (x, inaguralFDist[x]), allWordsFromInagural))\n",
    "\n",
    "topTen = sorted(topTen.items(), key=lambda x: x[1], reverse = True)\n",
    "\n",
    "topTenWord = list()\n",
    "for top in topTen[:10]:\n",
    "    topTenWord.append(top[0])\n",
    "\n",
    "print('Top 10 frequently used words across all the text files are: \\n', topTenWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency Distribution of Top 10 words whose character length in > 7 \n",
      "\n",
      "government: 600 \n",
      " citizens: 247 \n",
      " constitution: 206 \n",
      " national: 157 \n",
      " congress: 130 \n",
      " interests: 115 \n",
      " political: 106 \n",
      " executive: 97 \n",
      " principles: 96 \n",
      " progress: 94 \n",
      " "
     ]
    }
   ],
   "source": [
    "print('Frequency Distribution of Top 10 words whose character length in > 7 \\n')\n",
    "for top10 in topTenWord:\n",
    "    print(top10 + ':', inaguralFDist[top10], '\\n', end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of synonyms of progress are:  19\n",
      "Synonyms are:  advancement progress progress progression procession advance advancement forward_motion onward_motion progress progression advance progress come_on come_along advance get_on get_along shape_up advance progress pass_on move_on march_on go_on build_up work_up build progress\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "def synonym_count_func(word):\n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for i in syn.lemmas():\n",
    "            synonyms.append(i.name())\n",
    "    print('Number of synonyms of', word, 'are: ', len(set(synonyms)))\n",
    "    print('Synonyms are: ', *synonyms)\n",
    "synonym_count_func('progress')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
